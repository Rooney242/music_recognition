#######################
### IMPORTANT LINKS ###
#######################

ISMIR research datasets: https://www.ismir.net/resources/datasets/

universidad potente en machine learning y musica: https://www.upf.edu/web/mtg/software-datasets


################
### DATASETS ###
################

We need them to have valence and arousal at least

AMG1608: https://amg1608.blogspot.com/
	1608 excerpts of 30 seconds, only features

emoMusic: https://cvml.unige.ch/databases/emoMusic/
	744 songs (45 seconds)

MOODetector:Bi-Modal: https://github.com/johnglover/modal
	133 excerpts

---------------------------------------------------------------

DEAP Dataset: http://www.eecs.qmul.ac.uk/mmv/datasets/deap/readme.html
	120 videos with valence, arousal and dominance

IADS: https://csea.phhp.ufl.edu/Media.html#midmedia
	111 sounds, valence, arousal and dominance
	present a form

DEAM dataset - The MediaEval Database for Emotional Analysis of Music: https://cvml.unige.ch/databases/DEAM/
	1802 excerpts with music
	impossible to download

################
### SOFTWARE ###
################

Essentia: http://essentia.upf.edu/

Gaia: https://github.com/MTG/gaia

SMS tools: https://www.upf.edu/web/mtg/sms-tools


##################
### REFERENCES ###
##################

-- PREVIOUS TFMs --

2010's state of music emotion recognition: 
	http://archives.ismir.net/ismir2010/paper/000045.pdf

music emotion maps in arousal-valence space: 
	https://www.researchgate.net/publication/307909024_Music_Emotion_Maps_in_Arousal-Valence_Space


++ VAD SPACE and emotions++
(valence, arousal and dominance) (also PAD pleassure, arousal and dominance)

review of machine learning for music emotion recognition, for introduction:
	https://www.researchgate.net/publication/254004106_Machine_Recognition_of_Music_Emotion_A_Review

use in intro for differenciating between categorical and dimensional approaches:
	https://www.tandfonline.com/doi/full/10.1080/24751839.2018.1463749

machine learning with VAD space:
	https://link.springer.com/article/10.1007/s13735-017-0128-9

++ DATASETS ++

Ideas for MER datasets:
	https://groups.google.com/a/ismir.net/g/community/c/QTaPVQcnsyo?pli=1

Paper using AMG1608 dataset:
	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7178058

Description of emomusic dataset and baselines:
	https://ibug.doc.ic.ac.uk/media/uploads/documents/cmm13-soleymani.pdf

++ SOFTWARE ++

Essentia main explanation: 
	https://repositori.upf.edu/bitstream/handle/10230/32252/essentia_ismir_2013.pdf?sequence=1&isAllowed=y

Essentia algorithm reference for music extractor:
	https://essentia.upf.edu/streaming_extractor_music.html

++ FEATURE SELECTION ++

Feature extraction description for having a wide variety of features:
It also have some relation between music and emotions
	https://www.researchgate.net/publication/346359767_Audio_Features_for_Music_Emotion_Recognition_a_Survey

++ SYNTHETIC FEATURES ++

Seeing references for synthetic features:
	https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.734.5247&rep=rep1&type=pdf

Seeing featuretools for development: 
	https://featuretools.alteryx.com/en/stable/index.html

++ MER (music emotion recognition) ++

general paper for emotion recognition, could serve as comparison:
	https://bura.brunel.ac.uk/bitstream/2438/13139/1/FullText.pdf

jointly prediction of valence, arousal and dominance
	https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1494.PDF

++ FEATURE PROCESSING TECHNIQUES ++ 

regression approach:
	https://ieeexplore.ieee.org/abstract/document/4432654?casa_token=0ibhCotCuCwAAAAA:bISQfLgvuTnSex3lIfjh5K-PmXWCa5c8NY9udO9iBxUTg26EYrhrfKen_l7fYoqHDmdpJaQuvg

using svm:
	https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.1582&rep=rep1&type=pdf

bag of words for music:
	https://ieeexplore.ieee.org/abstract/document/7458322?casa_token=IJ5XiV5kucYAAAAA:M96keX2INB9q86qkGKRVIc45X3-G0clHCtS1E2X4fjWdsB_kbpuSXbvGO85DEINGOQR_ynfp2Q

bag of words for sentiment:
	https://ieeexplore.ieee.org/abstract/document/8462660?casa_token=S53Dpof0UdgAAAAA:MZq_pD3lV-zVPq2u12sPaSf6zmrewm92NYFmGIzV7yVmaFu2VJq6EmgZEJ6EOwe8huRcSIwBsQ


###############
### SUMMARY ###
###############
title: Music emotion recognition: synhetic features approach


Music and emotions have always been tightly related. Music emotion recognition (MER) tries to deal with how listening to music can induce different kind of emotions and in which way. How music brings about and emotional response is a subject of high interest and can have multiple applications apart from the merely music-emotion relation: music therapy, music personalization, music generation and more.

Using the valence-arousal representation of emotions and adding a dominance dimension to even differenciate better emotions, we can predict using different machine learning techniques and supervised trainning emotions in music. Knowing how music affects this three values on their own and altogeer can serve us to improve current models. Moreover, due to the subjectivity in classifiying emotions and the loss of interpretability when using some machine learning techniques, we can use synthetic feature to extract a better classification. The comprenhension of the low and high level features we perceive from music and how them affect valence, arousal and dominance is essential to carry out this task. Therefore, an investigation work has to be done to explore this potential improvement in music emotion recognition systems.


The common approach to design an automatic MER system with machine learning is to represent emotions in terms of three real-valued scores, valence, arousal and dominance, and then train a machine learning model able to predict those two scores from features extracted from the audio signals. The recent advances in deep learning have brought approaches in which the raw audio signals, or the spectrograms are directly fed into these deep learning models, and the deep learning model performs the automatic feature extraction and the construction of the model. These approaches achieve outstanding performances in terms of the accuracy of their predictions, but with significant limitation of the interpretability of the models by humans. On the other hand, the more classic approaches involving training automatic regressors like linear models or Gaussian Processes trained with high level, music-theory-based features are easier to interpret in terms of the role of each feature in the identification of the emotions. 

This thesis explores a new approach that aims to enable the control of the trade-off between the interpretability and the accuracy of such systems by training automatic regressors like linear models with synthetic features resulting from systematic compositions of the high level, music-theory based features.